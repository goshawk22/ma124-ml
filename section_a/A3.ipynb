{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6af362-6450-4459-befd-3e9100c36270",
   "metadata": {},
   "source": [
    "### Task A3 Machine Learning for binary classification using Logistic Regression\n",
    "\n",
    "Here you will start by running some code provided to you which will create a dataset. You will use this to train a Logistic Regression classification model. \n",
    "\n",
    "The dataset consists of a set of points $(x_0, x_1)$ in the plane each of which is either blue or red. Your model will take as input the coordinates of a point in the plane and return 0 if the point is likely to be red, or 1 if the point is likely to be blue. In practice your model will output a float (real number) between 0 and 1 which is the probability for the point to be blue, and you will then use 0.5 as a threshold to predict the point as being blue or red.\n",
    "\n",
    "**You are guided through this task below. You will see code cells that you need to run and code cells that you need to edit/complete. You can also add new code/markdown cells as required. You may wish to add markdown cells to comment on your code or to comment on any outputs you see.**\n",
    "\n",
    "#### Create and visualise the dataset\n",
    "\n",
    "**Run the cell below to create a planar dataset.** A visualisation of this is provided, you will see that it looks like a flower with some red points and some blue points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9751989-0013-4736-ace8-3d4db5dec595",
   "metadata": {},
   "source": [
    "You will see that you have some red points (which have label $y=0$) and some blue points (which have label $y=1$).\n",
    "\n",
    "More precisely you have created two vectors `X` and `y` (shorthands for $X_{\\text{train}}$ and $y_{\\text{train}}$) containing 400 training examples:\n",
    "- The numpy-array `X` of dimensions (2, 400), every column of `X` is the feature vector for a single training example and contains the coordinates of the point: $\\begin{pmatrix} x_0 \\\\ x_1\\end{pmatrix}$.\n",
    "- The numpy-array `y` of dimensions (1,400), every column of `y` contains the label for a single training example (0 for red, 1 for blue). \n",
    "- The columns of `X` correspond to the columns of `y`, i.e. the point in the first column of `X` corresponds to the first entry in `y` and so on.\n",
    "\n",
    "Your goal is to build a model to fit this data. In other words, you want the model to define regions of the plane as either red or blue. Red regions will correspond to points $(x_0,x_1)$ where the model returns a value less than 0.5 and with any other parts of the plane being blue regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211e20f-a4f5-48ac-8dc4-52f4fbc1cbbe",
   "metadata": {},
   "source": [
    "Let's start by building a logistic regression model and then measuring its performance on this problem. You will build it in numpy from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61b9a8-4f1b-4ca2-9f09-1aa60e9ecc76",
   "metadata": {},
   "source": [
    "These are the steps to build your model.\n",
    "\n",
    "1. Define the structure:\n",
    "$$\n",
    "\\hat y = \\sigma\\left( W \\cdot x+b \\right)=\\sigma\\left(\\begin{pmatrix} w_0 & w_1\\end{pmatrix}\\cdot \\begin{pmatrix}x_0 \\\\ x_1\\end{pmatrix}+b\\right).\n",
    "$$\n",
    "Here the notation is the same as the one in the lecture presentation for this project. In this case, for each example, we take a linear combination $w_0x_0+w_1x_1+b$ of the input features $x_0$ and $x_1$), where $w_0,w_1,b$ are real numbers, followed by a *sigmoid* activation function $\\sigma(\\lambda)=\\displaystyle\\frac{1}{1+\\exp(-\\lambda)}$.  The output $\\hat y$ is the model's predicted probability that the point $x=\\begin{pmatrix}x_0 \\\\ x_1\\end{pmatrix}$ is blue. Here $W = \\begin{pmatrix} w_0 & w_1\\end{pmatrix}$ is known as the weight matrix and $b$ is the bias vector (which in this special case will actually be a single value, a $1\\times 1$ matrix).\n",
    "\n",
    "2. Initialize the model's trainable parameters $W$ and $b$.\n",
    "\n",
    "3. The training loop is then as follows:\n",
    "    - Implement forward propagation:  compute an array whose entries are $\\hat{y}^{(i)}$ for each training example, with the current values of $W$ and $b$. Here $\\hat{y}^{(i)}$ is the output probability for example in column $i$ of $X$.\n",
    "    - Compute the cost $J$ for the current values of the parameters.\n",
    "    - Implement backward propagation to get the gradients, i.e. the rate of change of the $J$ with respect to each of $w_0,w_1$ and $b$ at their current values.\n",
    "    - Update the parameters using these gradients. This is known as gradient descent.\n",
    "\n",
    "In practice, you'll build helper functions for each of these steps, and then merge them into one function called `model()`. Once you've built `model()` and used it to optimise the parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b6b1c-3fb7-4e2c-bfc9-d0fe3eab4864",
   "metadata": {},
   "source": [
    "#### Initialize parameters\n",
    "\n",
    "**Complete the function `initialize_parameters` started below.** It takes in the dimension of the input layer `n_X` and the dimension of the output layer `n_y` and returns returns a Python dictionary `parameters`. The dictionary `parameters` contains the initialized parameters of the model: the weight matrix `W` and the bias vector `b`.\n",
    "\n",
    "Note that `n_X = 2` in our example, each point has two input coordinates,  and the dimension of the output layer is `n_y = 1`. This means that `W` will be a $1\\times 2$ matrix and `b` will be a $1\\times 1$ matrix for our example. However we would like our function to work more generally, and be in a form which can be adapted for use later on in this project, so we'd like the user to be able to specify `n_X` and `n_y` as inputs to `initialize_parameters`.\n",
    "\n",
    "- Use: `np.random.randn(n,m) * 0.01` to randomly initialize a matrix of the required shape for `W`.\n",
    "- Initialize the bias vector `b` so that all of its entries are zero.\n",
    "- Return both of these in a dictionary called `parameters` with names *W* and *b* respectively.\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b687b84-c477-4254-8bac-04041489a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_X, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_X -- integer\n",
    "    n_y -- integer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W -- weight matrix\n",
    "                    b -- bias vector\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize weight matrix nd bias vector\n",
    "    W = None #you need to replace \"None\" with appropriate code\n",
    "    b = None #you need to replace \"None\" with appropriate code\n",
    "\n",
    "    #Store W and b in a dictionary parameters\n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1254334-e636-4783-b7cb-cbed5cefe312",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Next you will implement forward propagation for logistic regression:\n",
    "\n",
    "$$\n",
    "z = WX + b \\text{ and then } \n",
    "\\hat y = \\sigma(z)\n",
    "$$\n",
    "\n",
    "As above, for $\\lambda\\in\\mathbb{R}$, $\\sigma(\\lambda)=\\displaystyle\\frac{1}{1+\\exp(-\\lambda)}$ (this is applied entry-wise to $z$ in the above). \n",
    "\n",
    "**Start by implementing the sigmoid function in numpy by replacing *None* in the code cell below with appropriate code. Then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46179e2c-a4e6-4df7-9b73-11edb95b38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    z -- input data, numpy array\n",
    "\n",
    "    Returns:\n",
    "    numpy array that contains sigmoid function applied to each\n",
    "    element of z.\n",
    "    \"\"\"\n",
    "    return None #you need to replace \"None\" with appropriate code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe8671-43ed-48bb-a24e-c1d7c12abd5c",
   "metadata": {},
   "source": [
    "**Complete the function `forward_propagation` started below.** It calculates the output of the logistic regression model, called `y_hat`, and returns (as a tuple) `y_hat` and `z` (in the notation given above).\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bf611-94fb-45d9-a5e1-0a62e12264d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data, numpy array of dimension (2, 400). Every column of X\n",
    "         is the feature vector for a single training example (input of the logistic regression)\n",
    "    parameters -- python dictionary containing the values of W and b to be used\n",
    "                  to compute the forward propagation.\n",
    "\n",
    "    Returns:\n",
    "    y_hat -- numpy array of dimension (1,400). Every column of y_hat is the\n",
    "             output probability of the logistic regression for that training example.\n",
    "    z -- numpy array of dimension (1,400), result of linear transformation before sigmoid activation is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve W and b from the dictionary \"parameters\"\n",
    "\n",
    "    # compute z\n",
    "\n",
    "    # Compute y_hat\n",
    "\n",
    "    return y_hat, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c721c0-6a4c-4a01-ad73-480a72e35a29",
   "metadata": {},
   "source": [
    "#### Compute cost\n",
    "\n",
    "Given the predictions $\\hat y^{(i)}$ on all the training examples, you can compute the cost $J$ as the average over all training examples of the losses:\n",
    "$$\n",
    "J = \\frac{1}{m} \\sum\\limits_{i = 0}^{m} L(\\hat y^{(i)}, y^{(i)})  .\n",
    "$$\n",
    "Here we are using the binary cross-entropy loss:\n",
    "$$\n",
    "L(\\hat y^{(i)}, y^{(i)}) = -\\large\\left(\\small y^{(i)}\\log\\left(\\hat y^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat y^{(i)}\\right)  \\large  \\right).\n",
    "$$\n",
    "\n",
    "Recall $y^{(i)}$ is the $i^{\\text{th}}$ component in $y$, i.e. the label for example $i$ and $\\hat{y}^{(i)}$ is the models current prediction of this label, the $i^{\\text{th}}$ component in $\\hat{y}$.\n",
    "\n",
    "**Complete the function `compute_cost()`, started below, to compute the value of the cost $J$:**\n",
    "\n",
    "1. First compute the vector of losses element-wise using the numpy arrays given as inputs `y_hat` and `y`.\n",
    "\n",
    "2. Sum the losses and divide by the number of training examples `m`. Call this value `cost`.\n",
    "\n",
    "3. Cast `cost` as type `float` using `float()` and return it.\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8dbe8-5879-496b-9d35-e83a84e71fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_hat -- numpy array, output of forward propagation\n",
    "    y -- numpy array of the same shape as y_hat, containing the true labels (0 for red, 1 for blue)\n",
    "\n",
    "    Returns:\n",
    "    cost -- a float which is the cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve number of training examples from the shape of y\n",
    "\n",
    "    # compute the vector of losses by computing the cross-entropy loss element-wise\n",
    "\n",
    "    # compute the total cost by averaging the loss over all training examples\n",
    "\n",
    "    # cast cost as a float\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5541a-a96d-4754-8376-08398cc4b002",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "You can now implement backward propagation. Note that you will complete the code cell below this, much of the code is already given to you but you need to stick with the notation established.\n",
    "\n",
    "NOTATION: in general for a generic numpy array `M` and a function $F$ with the entries of `M` as inputs, denote by `dM` the numpy array representing the gradient of $F$ with respect to `M`. More precisely `dM` contains the partial derivatives of $F$ with respect to the entries of `M`. For example if\n",
    "$$M = \\begin{pmatrix} a_{0,0} & a_{0,1} & a_{0,2} \\\\ a_{1,0} & a_{1,1} &  a_{1,2}\\end{pmatrix}$$ and $F$ is a function of $a_{0,0},a_{0,1},a_{0,2},a_{1,0},a_{1,1},a_{1,2}$ then $$dM = \\begin{pmatrix} \\frac{\\partial F}{\\partial a_{0,0}} & \\frac{\\partial F}{\\partial a_{0,1}} & \\frac{\\partial F}{\\partial a_{0,2}} \\\\ \\frac{\\partial F}{\\partial a_{1,0}} & \\frac{\\partial F}{\\partial a_{1,1}} & \\frac{\\partial F}{\\partial a_{1,2}}\\end{pmatrix}.$$\n",
    "\n",
    "Throught this project $F$ will be the cost function $J$.\n",
    "\n",
    "**Complete the function `backward_propagation()` started below.** It takes in a dictionary `parameters` containing the current parameters, `y_hat`,`X` and `y` and it returns a dictionary containing the gradients `dW` and `db` of the cost function with respect to the trainable parameters `W` and `b`. \n",
    "\n",
    "You are given the key code for calculating `dW` and `db`, it is, in the notation already established.\n",
    "\n",
    "`dz = y_hat - y\n",
    "dW = np.dot(dz, X.T)/m\n",
    "db = np.sum(dz, axis=1, keepdims=True)/m`\n",
    "\n",
    "You will see these lines of code in the cell below.\n",
    "\n",
    "**Once you have completed the code run this cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf7816-8617-4722-9fee-184939df87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, y_hat, X, y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing W and b to use\n",
    "    y_hat - from output of forward prop\n",
    "    X -- input data\n",
    "    y -- labels vector\n",
    "\n",
    "    Returns:\n",
    "    grads -- python dictionary containing the gradients\n",
    "    \"\"\"\n",
    "    # Retrieve the number of training examples from the shape of y\n",
    "    m = None #you need to replace None here.\n",
    "\n",
    "    # retrieve W from the dictionary \"parameters\".\n",
    "    W = None #you need to replace None here.\n",
    "\n",
    "    # Backward propagation: calculate dW, and db.\n",
    "    dz = y_hat - y\n",
    "    dW = np.dot(dz, X.T)/m\n",
    "    db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "\n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f968289-ebf6-4394-a84b-4b0c9f0753a4",
   "metadata": {},
   "source": [
    "#### Update parameters\n",
    "\n",
    "Next you will define a function to update parameters using gradient descent. In a similar way to that seen in task A2,\n",
    "\n",
    "1. $W$ will be updated to\n",
    "\n",
    "$$W - dW\\times \\text{learning rate}.$$\n",
    "\n",
    "and\n",
    "\n",
    "2. $b$ will be updated to$$b - db\\times\\text{learning rate}.$$\n",
    "\n",
    "**Complete the function `update_parameters()` started below.** It takes in the dictionary `parameters` containing the current parameters `W` and `b`, the dictionary `grads` containing the current gradient `dW`,`dB` and the learning rate (which defaults to the value $1.2$). **Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0ad32-a310-479a-a4f7-d4b009da3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters\n",
    "    grads -- python dictionary containing the gradients\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing the updated parameters\n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W\n",
    "    \n",
    "    W = copy.deepcopy(parameters[\"W\"])\n",
    "    b = None # you need to replace 'None' here\n",
    "\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "\n",
    "    # Update each parameter\n",
    "\n",
    "    # Store the new parameters in the dictionary parameters\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398b9c0-db5a-4020-b9e6-a3053452b824",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Now you will put everything together into a function `model` where you will apply `update_parameters` iteratively to update the parameters a number of times.\n",
    "\n",
    "**Complete the function `model` started below.** It takes as inputs the training data `X`, `y`, a positive integer `num_iterations` which is the number of times the parameters will be updated (defaulting to $10,000$) and a boolean `print_cost` which, when set to `True`, will mean the function prints out some costs as the values of the parameters are updated). `model` return a dictionary containing the optimised parameters at the end of this process. **Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d8d23-5b92-4194-a227-222e60998968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- numpy array, input data\n",
    "    y -- numpy array, true labels\n",
    "    num_iterations -- number of iterations in the gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary which contains the optimized parameters\n",
    "                  learnt by the model.\n",
    "    \"\"\"\n",
    "    # Optional: to control the random seed of the initialization uncomment next line:\n",
    "    # np.random.seed(3)\n",
    "\n",
    "    # retrieve n_x and n_y from X and y\n",
    "\n",
    "    # Initialize parameters\n",
    "\n",
    "    # Training loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        # Compute the cost\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        # Update parameters (use learning_rate = 1.2)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    learned_parameters = parameters\n",
    "    \n",
    "    # return learned parameters\n",
    "    return learned_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6aea6-c5d3-4b5f-8912-f5e1e01f0784",
   "metadata": {},
   "source": [
    "#### Predict on new data\n",
    "\n",
    "Below you are given a complete function `predict` that uses the learned parameters (output of `model`) and the `forward_propagation` function to predict a class for each example in the input matrix `X`. **Run the code cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a477f1-8b79-42f8-8d62-a439bc8b4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(learned_parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the learned parameters given by model\n",
    "    X -- numpy array input data\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions for each column of X (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute predicted probabilities using the learned parameters\n",
    "    \n",
    "    y_hat, z = forward_propagation(X, learned_parameters)\n",
    "\n",
    "    # Classify as 0 or 1 using 0.5 as a threshold\n",
    "    \n",
    "    predictions = (y_hat > 0.5)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864be32-6ab5-4b78-8213-53b95db139cc",
   "metadata": {},
   "source": [
    "#### Plot the results and calculate accuracy\n",
    "\n",
    "You are given the complete function `plot_decision_boundary` below which plots the decision boundary of the trained model along with the original data. It takes as arguments the `learned_parameters` (as output by the `model` function), the model, the training examples `X` and the true labels `y`. **Run the code cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08df84-42ca-4031-b352-b12a77a1ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision boundary\n",
    "\n",
    "def plot_decision_boundary(learned_parameters, X, y):\n",
    "\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = predict(learned_parameters,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x1')\n",
    "    plt.xlabel('x0')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed90b33-8ee3-445b-9a72-662af27b97e8",
   "metadata": {},
   "source": [
    "Now you can train your model using the training examples, and plot the decision boundary for the learned parameters, alongside the orginal data. **Run the code cell below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388eb28f-bb03-4fa0-8068-e4a1a19e9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model\n",
    "learned_parameters = model(X, y, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(learned_parameters, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb09a3-e070-401b-8ac7-d7d670255524",
   "metadata": {},
   "source": [
    "As you should see logistic regression doesn't do a good job in predicting the labels for this dataset. This is because the dataset is not linearly separable, and therefore a simple logistic regression performs poorly on this data. \n",
    "\n",
    "**Complete the code cell below with a formula for the accuracy of the model**. This should be the precentage of points for which the model gives the correct classification. **Then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f33c58-72d7-435c-87ef-462d76ccfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "\n",
    "predictions = predict(learned_parameters, X)\n",
    "\n",
    "accuracy = None # replace 'None' with code which calculates the percentage of points for\n",
    "            # which the prediction agrees with the actual label.\n",
    "    \n",
    "print ('Accuracy of logistic regression: %d ' % accuracy +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
