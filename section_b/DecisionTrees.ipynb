{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Edward King\n",
    "\n",
    "In this section I will explore the Decision Tree Model of machine learning.\n",
    "\n",
    "### Contents:\n",
    "\n",
    "* Graph theory\n",
    "* Decision trees\n",
    "* Information theory\n",
    "* Practicalities\n",
    "* The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 1 - Graph theory\n",
    "---\n",
    "\n",
    "### 1.1 - Graph theory basics ###\n",
    "\n",
    "Graph theory is the field of mathematics dedicated to studying the structure and properties of graphs. In order to understand Decision Trees, we first need to know what a tree is. In this section, I will provide the definitions needed to rigourously define a tree.\n",
    "\n",
    ">#### Definition 1.1: Graphs, Nodes, Arcs and Direction\n",
    ">A graph is an ordered pair of sets $(V,\\ E)$ where:\n",
    ">* $V$ is a set of nodes (or vertices)\n",
    ">* $E$ is a set of arcs (or edges) which are ordered pairs of vertices, i.e. $E=\\{(x,\\ y):x,y\\in V\\}$.\n",
    ">\n",
    ">It is important to note that arcs are directional: $(x,\\ y)\\neq(y,\\ x)$.  \n",
    ">We say a graph is directed if $\\exist (x,\\ y)\\in E\\ s.t.\\ (y,\\ x)\\notin E$.  \n",
    ">A graph is undirected if $\\forall (x,\\ y)\\in E,\\ \\exist (y,\\ x)\\in E$.\n",
    "\n",
    "![alt text](images/LetteredGraph.png)  \n",
    "_Figure 1.2 - an undirected graph._\n",
    "\n",
    "By convention, if a graph is directed it is shown using arrows on the arcs. Therefore, we may assume that this graph is undirected.\n",
    "\n",
    "Next we need a notion of cycles in a graph. To construct this, we need a series of definitions.\n",
    "\n",
    ">#### Definition 1.3: Walks, Trails, Paths, Closure, Cycles and Connectedness\n",
    ">* A walk is an ordered collection of nodes, $(x_1,\\ x_2,\\ x_3,\\ldots,\\ x_n)$.  \n",
    ">Every walk has a corresponding unique collection of arcs, $(e_1,\\ e_2,\\ldots,\\ e_{n-1})$ where $e_i = (x_i,\\ x_{i+1})$.  \n",
    ">N.B. we only consider walks of finite length.\n",
    ">\n",
    ">* A trail is a walk where the collection of arcs has no repeats i.e. $i\\neq j\\Rightarrow e_i\\neq e_j,\\ \\forall i,j: 0≤i,j≤n-1$.\n",
    ">\n",
    ">* A path is a trail where the collection of nodes has no repeats i.e. $i\\neq j\\Rightarrow x_i\\neq x_j,\\ \\forall i,j: 0≤i,j≤n-1$.\n",
    ">\n",
    ">* A walk is closed if $x_1=x_n$.\n",
    ">\n",
    ">* A cycle is a closed path.\n",
    ">\n",
    ">* A graph is cyclic if it has a cycle.\n",
    ">\n",
    ">* A graph is acyclic if it has no cycles.\n",
    ">\n",
    ">* Two nodes, $x_a,\\ x_b$, are connected if there is a walk of the form $(x_a,\\ x_i,\\ x_{i+1}, \\ldots,\\ x_b)$.\n",
    ">\n",
    ">* A graph is connected if each pair of nodes ($x,\\ y\\in V$) is connected.\n",
    "\n",
    "Visually we can see that there are multiple cycles in the graph in figure 1.2, e.g. $(A,\\ B,\\ C,\\ E,\\ A)$ and $(A,\\ D,\\ E,\\ A)$, meaning that this graph is cyclic.\n",
    "\n",
    ">#### Definition 1.4: Trees, Roots, Parents, Children, Leaves and Binary Trees\n",
    ">* A tree is a connected, acyclic graph.\n",
    ">\n",
    ">* In a tree, we designate one node to be the root.\n",
    ">\n",
    ">* Given two nodes $x,\\ y\\in V$, if there is a path from $x$ to the root which passes through $y$ then we say that $y$ is the parent node of $x$, the child node.\n",
    ">\n",
    ">* If a node has no children then it is a leaf node.\n",
    ">\n",
    ">* If each node in a tree has no more than two children then it is a binary tree.\n",
    "\n",
    "![alt text](images/trees.png)  \n",
    "_Figure 1.5 - three trees (the one on the right is a binary tree)._\n",
    "\n",
    "N.B. Only nodes with at most two arcs can be designated as the root of a binary tree however any node can be designated the root of a non-binary tree.\n",
    "\n",
    "At last we have all the pieces needed to start on decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 2 - Decision Trees\n",
    "---\n",
    "\n",
    "### 2.1 - Decision tree basics\n",
    "A decision tree acts on a set of data where each data point is given by an ordered set $(a_1,\\ a_2,\\ \\ldots ,\\ a_n,\\ c)$ where each $a_i$ is a feature and $c$ is the classification of this data point. One of the main advantages of using a decision tree is that there is no pre-processing of data as each feature can be of any data type (i.e. continuous, discrete), though for simplicity I will assume that the data features are all continuous in my code.\n",
    "\n",
    "A decision tree is a tree (or tree-like structure) where each node in the set $V$ describes a condition $C$ on a data feature (for example: $a_1\\leq 1$, or $a_2 =$ \"Honda\") which indicates how data should be split as you traverse the tree (with the given examples: feature $a_1$ is less than or equal to $1$ or $a_2$ is \"Honda\"). If the node is a leaf node then the condition states what classification should be assigned to the data point.\n",
    "\n",
    "![alt text](images/DecisionTree.png)  \n",
    "_Figure 2.1 - A simple decision tree_\n",
    "\n",
    "Consider the well known model, the Galton board, shown in figure 2.2 below. Consider each ball to be a data point which traverses the decision tree by falling under gravity. The pocket it falls into at the bottom determines which class it is assigned to. Each peg in the board represents a node in the decision tree that decides which path the data point should take down the board.\n",
    "\n",
    "![alt text](images/GaltonBoard.png)  \n",
    "_Figure 2.2 - Galton board_\n",
    "\n",
    "### 2.2 - Decision trees in the wild\n",
    "This definition allows for a wide range of use cases beyond the one detailed later. This concept is used frequently in game development to design the behaviour of non-player characters and enemies. This is clear in the board game Gloomhaven where players are given a decision tree to determine how the enemies move and attack the players. Each enemy has its own set of information (such as whether or not it is stunned, its position on the board, etc.) which can be considered as the features $(a_1,\\ a_2,\\ \\ldots ,\\ a_n)$ and the classification, $c$, is the action the enemy will take. The image below could be formalised to look more like figure 2.1 but I shall leave this as an exercise to the reader.\n",
    "\n",
    "![alt text](images/gloomhaven_behaviour_tree.png)  \n",
    "_Figure 2.3 - Gloomhaven behaviour tree_\n",
    "\n",
    "### 2.3 - Back to machine learning ###\n",
    "Now we return to the meat of this section: how decision trees are applied to machine learning. I will explore the \"learning\" as it applies here in the next section but first I will discuss some reasons why one might use decision trees over other models.\n",
    "\n",
    "Advantages:\n",
    "* Unlike other models (particularly neural networks), it is very easy to see how a decision tree comes to its conclusions.\n",
    "* As mentioned in §2.1, the data features can be either discrete or continuous.\n",
    "* Can be used in both classification and regression problems (though I'm only considering classification problems here).\n",
    "* Fast query time - on average this has complexity $O(\\log{n_{samples}})$<sup>[[1]](https://scikit-learn.org/stable/modules/tree.html)</sup>.\n",
    "\n",
    "Disadvantages:\n",
    "* Use of a greedy approach (the best choice is chosen at each stage without considering the effect on future choices). This can lead to a non-optimal solution but will be faster than a more comprehensive search.\n",
    "* Small variations in the input data can lead to wildly different trees.\n",
    "* Slow construction time - on average this has complexity $O(n_{samples}n_{features}\\log(n_{samples}))$<sup>[[1]](https://scikit-learn.org/stable/modules/tree.html)</sup>. This issue can be mitigated since we only construct the tree once then query it from then on.\n",
    "* Cannot add data to the tree once constructed - unlike KNN, our model cannot be improved once constructed.\n",
    "* Prone to overfitting - (see Tobey's section for a more detailed description of the issue). Some remedies to this are discussed at the end of §2.4.\n",
    "\n",
    "\n",
    "### 2.4 - Algorithm for constructing a decision tree ###\n",
    "Now that we know what a decision tree looks like, can we construct one? Below I detail the algorithm for constructing a decision tree, italicised lines will be expanded upon later:\n",
    ">1. Receive a dataset. _Select stopping requirements_.\n",
    ">\n",
    ">2. Create a root node. Mark it and the whole dataset as active.\n",
    ">\n",
    ">3. If the stopping requirements have been met, proceed to step 10, otherwise proceed to step 4.\n",
    ">\n",
    ">4. _Choose a condition on which to split the active dataset_. If the condition has negative information gain, proceed to step 10.\n",
    ">\n",
    ">5. Assign the splitting condition to the active node and split the active dataset into left and right subsets.\n",
    ">\n",
    ">6. Create a left child node and mark the left subset as active.\n",
    ">\n",
    ">7. Mark the left node as active and proceed to step 3.\n",
    ">\n",
    ">8. Create a right child node and mark the right subset as active.\n",
    ">\n",
    ">9. Mark the right node as active and proceed to step 3.\n",
    ">\n",
    ">10. Assign the mode class among the active dataset to the active node.\n",
    ">\n",
    ">11. If the active node is a left node, proceed to step 8. Otherwise proceed to step 12.\n",
    ">\n",
    ">12. _If there are incomplete nodes, proceed to step 3 with the first incomplete node as active._ Othewise, proceed to step 13.\n",
    ">\n",
    ">13. Terminate the algorithm.\n",
    "\n",
    "\n",
    "This algorithm can be optimised using recursion (which I use in my code), though for clarity I use this slightly more inefficient version here (this optimisation is relatively small as the majority of the complexity comes from step 4 as will be discussed later).\n",
    "\n",
    "As promised, I will expand on steps 1 and 11 now. Step 4 will be left for its own section.\n",
    "\n",
    "Step 1 is used as a method of preventing overfitting as well as enabling termination of the algorithm. There are three requirements which may be met in order to terminate the algorithm:\n",
    "1. Maximum distance from the root - if the tree has too many nodes.\n",
    "2. Minimum sample size - if the data subset assigned to a node has too few elements.\n",
    "3. Non-positive information gain - if the best splitting condition has negative information gain (to be explained in the next chapter).\n",
    "\n",
    "If either of these conditions is satisfied, then the algorithm is terminated.\n",
    "\n",
    "With regard to step 11, there is an algorithm to determine the next node (depth first search) but this is not necessary as using the recursive algorithm solves this.\n",
    "\n",
    "I will now explain step 4 of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3 - Choosing Optimal Splitting Conditions\n",
    "---\n",
    "\n",
    "### 3.1 - Information theory ###\n",
    "Step 4 says: \"Choose a condition on which to split the data assigned to the active node\". We want to choose the \"best\" condition to do this so one might compare all possible conditions to find the best one. However, given two or more choices of splitting conditions, how do we evaluate which is more effective at splitting the data? What does \"effective\" even look like? For this problem, we need two concepts from information theory: the study of communication, storage and quantification of information.\n",
    "\n",
    "### 3.2 - Impurity ###\n",
    "\n",
    "Impurity is the measure of uncertainty in an event. For our purposes, impurity measures the uncertainty of picking a class from the dataset.\n",
    "\n",
    "The first piece we need is to know the impurity of our dataset. There are two measures of impurity that I will use here: entropy and gini index. The respective formulae used to calulate them are<sup>[[2]](https://www.ibm.com/topics/decision-trees)</sup>:\n",
    ">\n",
    "> \\begin{align*}  \n",
    "> \\text{E}(D) = \\sum_{c} -p_c\\ \\log(p_c)\\\\\n",
    "> \\text{G}(D) = 1 - \\sum_{c} p_c^2\n",
    "> \\end{align*}\n",
    "\n",
    "Where:\n",
    "* $D$ is the dataset that is having its impurity calculated\n",
    "* $p_c$ is the proportion of class $c$ in the dataset\n",
    "\n",
    "N.B. The choice of the base of the logarithm changes the type of entropy used. For machine learning we use $\\log_2$. This is known as Shannon entropy. In other fields it is common to use base $10$ or the natural logarithm.\n",
    "\n",
    "### 3.3 - Information gain ###\n",
    "\n",
    "Information gain is the measure of reduction in impurity of a dataset.\n",
    "\n",
    ">The formula to calculate information gain is<sup>[[2]](https://www.ibm.com/topics/decision-trees)</sup>:  \n",
    ">\\begin{align*}\n",
    ">\\text{IG}(C) = \\text{Im}(P) - \\sum_{\\chi}\\text{Im}(\\chi)\n",
    ">\\end{align*}\n",
    "\n",
    "Where:\n",
    "* $C$ is the condition that is having its information gain calculated\n",
    "* $\\text{Im}$ is the impurity measure being used\n",
    "* $P$ is the parent dataset\n",
    "* $\\chi$ is the child dataset\n",
    "\n",
    "\n",
    "Whichever splitting condition has the highest information gain is the one that should be used.\n",
    "\n",
    "### 3.4 - Step 4 algorithm ###\n",
    "\n",
    ">1. Order the data features and find all unique values taken by the dataset for each feature (i.e. discard repeated values within a single feature).\n",
    ">\n",
    ">2. Mark all data features and unique values as unconsidered.\n",
    ">\n",
    ">3. Set the highest information gain to $-\\infty$.\n",
    ">\n",
    ">4. Select an unconsidered data feature.\n",
    ">\n",
    ">5. Select an unconsidered value for the current feature.\n",
    ">\n",
    ">6. Calculate the information gain resulting from splitting the current data feature at the current value.\n",
    ">\n",
    ">7. If the gain is greater than the highest information gain, proceed to step 8. Otherwise proceed to step 11.\n",
    ">\n",
    ">8. Change the mark on the feature, value pair that was marked highest to discounted.\n",
    ">\n",
    ">9. Mark the current feature and value as highest.\n",
    ">\n",
    ">10. Set the highest information gain to the calculated value.\n",
    ">\n",
    ">11. If there are unconsidered values for the current feature, proceed to step 5. Otherwise proceed to step 12.\n",
    ">\n",
    ">12. If there are unconsidered features, proceed to step 4. Otherwise proceed to step 13.\n",
    ">\n",
    ">13. Return the highest information gain and terminate the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4 - The Code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I import the libraries that I will use.  \n",
    "* Numpy for manipulating arrays<sup>[[3]](https://numpy.org/doc/stable/)</sup>\n",
    "* Pandas for importing large datasets<sup>[[4]](https://pandas.pydata.org/docs/)</sup>\n",
    "\n",
    "\n",
    "N.B. I do use the scikit learn library later on<sup>[[5]](https://scikit-learn.org/stable/modules/classes.html)</sup>. Specifically, I use it to get a random sample from my dataset and to check how accurate my model was.\n",
    "\n",
    "Next I import the datasets to be used on the model.\n",
    "\n",
    "I have included both the iris dataset<sup>[[6]]()</sup> and the dry bean dataset<sup>[[7]](http://archive.ics.uci.edu/dataset/602/dry+bean+dataset)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30279</td>\n",
       "      <td>634.927</td>\n",
       "      <td>212.560556</td>\n",
       "      <td>181.510182</td>\n",
       "      <td>1.171067</td>\n",
       "      <td>0.520401</td>\n",
       "      <td>30600</td>\n",
       "      <td>196.347702</td>\n",
       "      <td>0.775688</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.943852</td>\n",
       "      <td>0.923726</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30477</td>\n",
       "      <td>670.033</td>\n",
       "      <td>211.050155</td>\n",
       "      <td>184.039050</td>\n",
       "      <td>1.146768</td>\n",
       "      <td>0.489478</td>\n",
       "      <td>30970</td>\n",
       "      <td>196.988633</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.984081</td>\n",
       "      <td>0.853080</td>\n",
       "      <td>0.933374</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.871186</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30519</td>\n",
       "      <td>629.727</td>\n",
       "      <td>212.996755</td>\n",
       "      <td>182.737204</td>\n",
       "      <td>1.165591</td>\n",
       "      <td>0.513760</td>\n",
       "      <td>30847</td>\n",
       "      <td>197.124320</td>\n",
       "      <td>0.770682</td>\n",
       "      <td>0.989367</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.925480</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.856514</td>\n",
       "      <td>0.998345</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30685</td>\n",
       "      <td>635.681</td>\n",
       "      <td>213.534145</td>\n",
       "      <td>183.157146</td>\n",
       "      <td>1.165852</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>31044</td>\n",
       "      <td>197.659696</td>\n",
       "      <td>0.771561</td>\n",
       "      <td>0.988436</td>\n",
       "      <td>0.954240</td>\n",
       "      <td>0.925658</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.856844</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30834</td>\n",
       "      <td>631.934</td>\n",
       "      <td>217.227813</td>\n",
       "      <td>180.897469</td>\n",
       "      <td>1.200834</td>\n",
       "      <td>0.553642</td>\n",
       "      <td>31120</td>\n",
       "      <td>198.139012</td>\n",
       "      <td>0.783683</td>\n",
       "      <td>0.990810</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>0.912125</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.831973</td>\n",
       "      <td>0.999061</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "5  30279    634.927       212.560556       181.510182      1.171067   \n",
       "6  30477    670.033       211.050155       184.039050      1.146768   \n",
       "7  30519    629.727       212.996755       182.737204      1.165591   \n",
       "8  30685    635.681       213.534145       183.157146      1.165852   \n",
       "9  30834    631.934       217.227813       180.897469      1.200834   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "5      0.520401       30600     196.347702  0.775688  0.989510   0.943852   \n",
       "6      0.489478       30970     196.988633  0.762402  0.984081   0.853080   \n",
       "7      0.513760       30847     197.124320  0.770682  0.989367   0.967109   \n",
       "8      0.514081       31044     197.659696  0.771561  0.988436   0.954240   \n",
       "9      0.553642       31120     198.139012  0.783683  0.990810   0.970278   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  \n",
       "5     0.923726      0.007020      0.003153      0.853270      0.999236  SEKER  \n",
       "6     0.933374      0.006925      0.003242      0.871186      0.999049  SEKER  \n",
       "7     0.925480      0.006979      0.003158      0.856514      0.998345  SEKER  \n",
       "8     0.925658      0.006959      0.003152      0.856844      0.998953  SEKER  \n",
       "9     0.912125      0.007045      0.003008      0.831973      0.999061  SEKER  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' If using the Iris data set use below: '''\n",
    "\n",
    "# col_names = ['sepal_length',\n",
    "#              'sepal_width',\n",
    "#              'petal_length',\n",
    "#              'petal_width',\n",
    "#              'type']\n",
    "\n",
    "# data = pd.read_csv('iris.data', sep=',', header=None, names=col_names)\n",
    "\n",
    "\n",
    "\n",
    "''' If using the dry bean dataset, use below: '''\n",
    "\n",
    "col_names = ['Area',\n",
    "             'Perimeter',\n",
    "             'MajorAxisLength',\n",
    "             'MinorAxisLength',\n",
    "             'AspectRation',\n",
    "             'Eccentricity',\n",
    "             'ConvexArea', \n",
    "             'EquivDiameter',\n",
    "             'Extent',\n",
    "             'Solidity',\n",
    "             'Roundness',\n",
    "             'Compactness',\n",
    "             'ShapeFactor1',\n",
    "             'ShapeFactor2',\n",
    "             'ShapeFactor3',\n",
    "             'ShapeFactor4',\n",
    "             'Class']\n",
    "\n",
    "data = pd.read_csv('Dry_Bean_Dataset.data', sep=',', header=None, names=col_names)\n",
    "\n",
    "\n",
    "''' Leave this uncommented regardless of the dataset used. '''\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a tree, we need to know what its nodes look like.  \n",
    "In the following block, I define the `Node` class which stores information about:\n",
    "* The data feature which is being split by this node.\n",
    "* The threshold value which is applied to the feature.\n",
    "* The node's left and right children (which may be `None`).\n",
    "* The class to which data points should be assigned (`None` if not a leaf node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_id: int = None, threshold = None, cls: str = None):        \n",
    "        self.feature_id = feature_id\n",
    "        self.threshold = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.cls = cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I write the class which will handle contruction and querying the decision tree.\n",
    "\n",
    "It has four attributes:\n",
    "* `max_depth` and `min_samples`: used to determine the stopping conditions as explained in §2.4.\n",
    "* `root`: used as a base to construct the tree from.\n",
    "* `imp_mode`: used to decide which impurity measure should be used.\n",
    "\n",
    "\n",
    "and ___ methods:\n",
    "* `__init__`: a constructor for the class.\n",
    "* `constructTree`: initialisation of the construction algorithm laid out in §2.4.\n",
    "* `setChildNodes`: the recursive function used to construct the tree.\n",
    "* `getBestSplit`: runs the algorithm from §3.5.\n",
    "* `calculateInfoGain`, `getEntropy` and `getGini`: all apply the respective formulae from §3.2 and §3.3.\n",
    "* `splitDataset`: splits the dataset according to a given according to the given conditions.\n",
    "* `getLeafValue`: gets the mode class in a dataset.\n",
    "* `printTree`: displays the tree in a readable format.\n",
    "* `predict`: runs `makePrediction` on each datapoint in a set.\n",
    "* `makePrediction`: searches the tree to determine the class of a given datapoint.\n",
    "\n",
    "N.B. When a variable must be defined but will not be used, I use an underscore: `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, max_depth: int = 3, min_samples: int = 4, imp_mode: str = 'entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "    \n",
    "        self.root = None\n",
    "\n",
    "        self.imp_mode = imp_mode\n",
    "\n",
    "\n",
    "\n",
    "    # This is the function that initiates the algorithm.\n",
    "    def constructTree(self, dataset):\n",
    "        # Split the root\n",
    "        _, num_features = np.shape(dataset[:,:-1])\n",
    "        feature_id, threshold, _ = self.getBestSplit(dataset, num_features)\n",
    "        self.root = Node(feature_id, threshold)\n",
    "\n",
    "\n",
    "        # Find the root's children\n",
    "        self.setChildNodes(self.root, dataset, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def setChildNodes(self, node, dataset, depth):\n",
    "        num_samples, num_features = np.shape(dataset[:,:-1])\n",
    "        depth += 1\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (depth < self.max_depth) and (num_samples > self.min_samples):\n",
    "            # Split child nodes\n",
    "            left_dataset, right_dataset = self.splitDataset(node.feature_id, node.threshold, dataset)\n",
    "\n",
    "            left_feature_id, left_threshold, left_info_gain = self.getBestSplit(left_dataset, num_features)\n",
    "            right_feature_id, right_threshold, right_info_gain = self.getBestSplit(right_dataset, num_features)\n",
    "\n",
    "            if left_info_gain > 0:\n",
    "                # Set left node\n",
    "                node.left = Node(left_feature_id, left_threshold)\n",
    "                \n",
    "                # Determine children of left node\n",
    "                self.setChildNodes(node.left, left_dataset, depth)\n",
    "\n",
    "            else:\n",
    "                # Set leaf value\n",
    "                node.left = Node(cls=self.getLeafValue(left_dataset[:, -1]))\n",
    "\n",
    "            if right_info_gain > 0:\n",
    "                # Set right node\n",
    "                node.right = Node(right_feature_id, right_threshold)\n",
    "\n",
    "                # Determine children of right node\n",
    "                self.setChildNodes(node.right, right_dataset, depth)\n",
    "\n",
    "            else:\n",
    "                # Set leaf value\n",
    "                node.right = Node(cls=self.getLeafValue(right_dataset[:, -1]))\n",
    "\n",
    "        else:\n",
    "            # Set leaf value if the algorithm stops\n",
    "            node.cls = self.getLeafValue(dataset[:, -1])\n",
    "    \n",
    "\n",
    "\n",
    "    def getBestSplit(self, dataset, num_features):\n",
    "        \n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        best_info = -float(\"inf\")\n",
    "        \n",
    "        # iterate over all features\n",
    "        for feature_id in range(num_features):\n",
    "            feature_values = dataset[:, feature_id]\n",
    "\n",
    "            # find the unique values for the chosen feature\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            # iterate over all unique values\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.splitDataset(feature_id, threshold, dataset)\n",
    "                \n",
    "                # check children are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    class_list, class_list_left, class_list_right = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    \n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.calculateInfoGain(class_list, class_list_left, class_list_right)\n",
    "                    \n",
    "                    # update the best split if necessary\n",
    "                    if curr_info_gain>best_info:\n",
    "                        best_feat = feature_id\n",
    "                        best_thresh = threshold\n",
    "                        best_info = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_feat, best_thresh, best_info\n",
    "    \n",
    "\n",
    "\n",
    "    # Implements the formulae from §3.3\n",
    "    def calculateInfoGain(self, class_list, class_list_left, class_list_right):\n",
    "        if self.imp_mode == 'entropy':\n",
    "            parent_imp = self.getEntropy(class_list)\n",
    "            left_imp = self.getEntropy(class_list_left)\n",
    "            right_imp = self.getEntropy(class_list_right)\n",
    "\n",
    "        elif self.imp_mode == 'gini':\n",
    "            parent_imp = self.getGini(class_list)\n",
    "            left_imp = self.getGini(class_list_left)\n",
    "            right_imp = self.getGini(class_list_right)\n",
    "\n",
    "        left_weight = len(class_list_left) / len(class_list)\n",
    "        right_weight = len(class_list_right) / len(class_list)\n",
    "\n",
    "        return parent_imp - left_weight * left_imp - right_weight * right_imp\n",
    "    \n",
    "\n",
    "\n",
    "    # Implement the formulae from §3.2\n",
    "    def getEntropy(self, class_list):\n",
    "        unique_classes = np.unique(class_list)\n",
    "        entropy = 0\n",
    "        for cls in unique_classes:\n",
    "            p_cls = len(class_list[class_list == cls]) / len(class_list)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def getGini(self, class_list):\n",
    "        unique_classes = np.unique(class_list)\n",
    "        gini = 0\n",
    "        for cls in unique_classes:\n",
    "            p_cls = len(class_list[class_list == cls]) / len(class_list)\n",
    "            gini += p_cls ** 2\n",
    "        return 1 - gini\n",
    "\n",
    "\n",
    "\n",
    "    # This splits the data according to a given condition.\n",
    "    def splitDataset(self, feature_id, threshold, dataset):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_id]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_id]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "\n",
    "\n",
    "    # Gets the mode class in a dataset.\n",
    "    def getLeafValue(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "\n",
    "\n",
    "    # Outputs the tree in a readable format.\n",
    "    def printTree(self, tree = None, indent: str = \" \", feat_names = None):\n",
    "        \n",
    "        # On the first call, starts at the root.\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If the node is a leaf node return its value.\n",
    "        if tree.cls is not None:\n",
    "            print(tree.cls)\n",
    "\n",
    "        else:\n",
    "            # Determines the output format.\n",
    "            if not feat_names:\n",
    "                print(\"X_\" + str(tree.feature_id), \"<=\", tree.threshold)\n",
    "\n",
    "            else:\n",
    "                print(str(feat_names[tree.feature_id]), \"<=\", tree.threshold)\n",
    "\n",
    "            # Continues to the child nodes.\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.left, indent + indent, feat_names)\n",
    "            \n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.right, indent + indent, feat_names)\n",
    "\n",
    "\n",
    "\n",
    "    # Predicts the outputs of a dataset.\n",
    "    def predict(self, X):\n",
    "        predictions = [self.makePrediction(x, self.root) for x in X]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "    # Predicts the class of a single datapoint.\n",
    "    def makePrediction(self, x, tree):\n",
    "\n",
    "        if tree.cls!=None:\n",
    "            return tree.cls\n",
    "        \n",
    "        feature_val = x[tree.feature_id]\n",
    "\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.makePrediction(x, tree.left)\n",
    "        \n",
    "        else:\n",
    "            return self.makePrediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I do some reshaping of the dataset and get a random selection of datapoints to construct the tree from.\n",
    "\n",
    "This is the first of two uses of the sklearn library in my section. As you can see, I do not use it to implement any of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=1000, test_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I run the code to construct the tree from the chosen dataset. In this example I use the gini index measure. I then display the tree with the `printTree` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perimeter <= 749.909\n",
      " left:AspectRation <= 1.3435720617876916\n",
      "  left:MinorAxisLength <= 181.66087987226535\n",
      "    left:Roundness <= 0.925011277935379\n",
      "        left:SIRA\n",
      "        right:DERMASON\n",
      "    right:ShapeFactor4 <= 0.9938135901063284\n",
      "        left:DERMASON\n",
      "        right:SEKER\n",
      "  right:ConvexArea <= 37483.0\n",
      "    left:MajorAxisLength <= 280.69309975606564\n",
      "        left:DERMASON\n",
      "        right:HOROZ\n",
      "    right:Roundness <= 0.9220680534983396\n",
      "        left:DERMASON\n",
      "        right:SIRA\n",
      " right:AspectRation <= 1.8514302867459431\n",
      "  left:MinorAxisLength <= 210.48639000515425\n",
      "    left:Roundness <= 0.8353305514534717\n",
      "        left:HOROZ\n",
      "        right:SIRA\n",
      "    right:Area <= 107911.0\n",
      "        left:CALI\n",
      "        right:BOMBAY\n",
      "  right:MinorAxisLength <= 215.48975027642743\n",
      "    left:ShapeFactor4 <= 0.998228338038105\n",
      "        left:HOROZ\n",
      "        right:HOROZ\n",
      "    right:Area <= 87055.0\n",
      "        left:CALI\n",
      "        right:BOMBAY\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=5, min_samples=5, imp_mode=\"gini\")\n",
    "\n",
    "dataset = np.concatenate((X_train, Y_train), axis=1)\n",
    "\n",
    "tree.constructTree(dataset)\n",
    "\n",
    "tree.printTree(feat_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I use the sklearn library once more to determine the accuracy of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tree.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5 - Potential Improvements\n",
    "---\n",
    "\n",
    "Aside from the inefficiencies resulting from my incompetence, there are a few methods that could be employed to improve my code.\n",
    "\n",
    "As mentioned in §2.3, decision trees can be used in both classification and regression and their inputs can be both discrete and continuous. My code currently only works with continuous data in a classification problem.\n",
    "\n",
    "In order to improve the effectiveness of the model, I could make use of concepts such as random forest (see Stavros' section) and xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "---\n",
    "\n",
    "[[1]](https://scikit-learn.org/stable/modules/tree.html) Scikit documentation for decision trees functions  \n",
    "[[2]](https://www.ibm.com/topics/decision-trees) IBM decision tree page  \n",
    "[[3]](https://numpy.org/doc/stable/) Numpy library  \n",
    "[[4]](https://pandas.pydata.org/docs/) Pandas library  \n",
    "[[5]](https://scikit-learn.org/stable/modules/classes.html) Scikit learn library  \n",
    "[[6]](http://archive.ics.uci.edu/dataset/53/iris) Iris dataset  \n",
    "[[7]](http://archive.ics.uci.edu/dataset/602/dry+bean+dataset) Dry bean dataset  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
