{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Edward King\n",
    "\n",
    "In this section I will explore the Decision Tree Model of machine learning.\n",
    "\n",
    "### Contents:\n",
    "\n",
    "* Graph theory\n",
    "* Decision trees\n",
    "* Information theory\n",
    "* Practicalities\n",
    "* The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 1 - Graph theory\n",
    "---\n",
    "\n",
    "### 1.1 - Graph theory basics ###\n",
    "\n",
    "Graph theory is the field of mathematics dedicated to studying the structure and properties of graphs. In order to understand Decision Trees, we first need to know what a tree is. In this section, I will provide the definitions needed to rigourously define a tree.\n",
    "\n",
    ">#### Definition 1.1: Graphs, Nodes, Arcs and Direction\n",
    ">A __graph__ is an ordered pair of sets $(V,\\ E)$ where:\n",
    ">* $V$ is a set of __nodes__ (or vertices)\n",
    ">* $E$ is a set of __arcs__ (or edges) which are ordered pairs of vertices, i.e. $E=\\{(x,\\ y):x,y\\in V\\}$.\n",
    ">\n",
    ">It is important to note that arcs are directional: $(x,\\ y)\\neq(y,\\ x)$.  \n",
    ">We say a graph is __directed__ if $\\exist (x,\\ y)\\in E\\ s.t.\\ (y,\\ x)\\notin E$.  \n",
    ">A graph is __undirected__ if $\\forall (x,\\ y)\\in E,\\ \\exist (y,\\ x)\\in E$.\n",
    "\n",
    "![alt text](images/LetteredGraph.png)  \n",
    "_Figure 1.2 - an undirected graph._\n",
    "\n",
    "By convention, if a graph is directed it is shown using arrows on the arcs. Therefore, we may assume that this graph is undirected.\n",
    "\n",
    "Next we need a notion of cycles in a graph. To construct this, we need a series of definitions.\n",
    "\n",
    ">#### Definition 1.3: Walks, Trails, Paths, Closure, Cycles and Connectedness\n",
    ">* A __walk__ is an ordered collection of nodes, $(x_1,\\ x_2,\\ x_3,\\ldots,\\ x_n)$.  \n",
    ">Every walk has a corresponding unique collection of arcs, $(e_1,\\ e_2,\\ldots,\\ e_{n-1})$ where $e_i = (x_i,\\ x_{i+1})$.  \n",
    ">N.B. we only consider walks of finite length.\n",
    ">\n",
    ">* A __trail__ is a walk where the collection of arcs has no repeats i.e. $i\\neq j\\Rightarrow e_i\\neq e_j,\\ \\forall i,j: 0≤i,j≤n-1$.\n",
    ">\n",
    ">* A __path__ is a trail where the collection of nodes has no repeats i.e. $i\\neq j\\Rightarrow x_i\\neq x_j,\\ \\forall i,j: 0≤i,j≤n-1$.\n",
    ">\n",
    ">* A walk is __closed__ if $x_1=x_n$.\n",
    ">\n",
    ">* A __cycle__ is a closed path.\n",
    ">\n",
    ">* A graph is __cyclic__ if it has a cycle.\n",
    ">\n",
    ">* A graph is __acyclic__ if it has no cycles.\n",
    ">\n",
    ">* Two nodes, $x_a,\\ x_b$, are __connected__ if there is a walk of the form $(x_a,\\ x_i,\\ x_{i+1}, \\ldots,\\ x_b)$.\n",
    ">\n",
    ">* A graph is __connected__ if each pair of nodes ($x,\\ y\\in V$) is connected.\n",
    "\n",
    "Visually we can see that there are multiple cycles in the graph in figure 1.2, e.g. $(A,\\ B,\\ C,\\ E,\\ A)$ and $(A,\\ D,\\ E,\\ A)$, meaning that this graph is cyclic.\n",
    "\n",
    ">#### Definition 1.4: Trees, Roots, Parents, Children, Leaves and Binary Trees\n",
    ">* A __tree__ is a connected, acyclic graph.\n",
    ">\n",
    ">* In a tree, we designate one node to be the __root__.\n",
    ">\n",
    ">* Given two nodes $x,\\ y\\in V$, if there is a path from $x$ to the root which passes through $y$ then we say that $y$ is the __parent node__ of $x$, the __child node__.\n",
    ">\n",
    ">* If a node has no children then it is a __leaf node__.\n",
    ">\n",
    ">* If each node in a tree has no more than two children then it is a __binary tree__.\n",
    "\n",
    "![alt text](images/trees.png)  \n",
    "_Figure 1.5 - three trees (the one on the right is a binary tree)._\n",
    "\n",
    "N.B. Only nodes with at most two arcs can be designated as the root of a binary tree however any node can be designated the root of a non-binary tree.\n",
    "\n",
    "At last we have all the pieces needed to start on decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 2 - Decision Trees\n",
    "---\n",
    "\n",
    "### 2.1 - Decision tree basics\n",
    "A decision tree acts on a set of data where each data point is given by an ordered set $(a_1,\\ a_2,\\ \\ldots ,\\ a_n,\\ c)$ where each $a_i$ is a feature and $c$ is the classification of this data point. One of the main advantages of using a decision tree is that there is no pre-processing of data as each feature can be of any data type (i.e. continuous, discrete), though for simplicity I will assume that the data features are all continuous in my code.\n",
    "\n",
    "A decision tree is a tree (or tree-like structure) where each node in the set $V$ describes a condition $C$ on a data feature (for example: $a_1\\leq 1$, or $a_2 =$ \"Honda\") which indicates how data should be split as you traverse the tree (with the given examples: feature $a_1$ is less than or equal to $1$ or $a_2$ is \"Honda\"). If the node is a leaf node then the condition states what classification should be assigned to the data point.\n",
    "\n",
    "![alt text](images/DecisionTree.png)  \n",
    "_Figure 2.1 - A simple decision tree_\n",
    "\n",
    "Consider the well known model, the Galton board, shown in figure 2.2 below. Consider each ball to be a data point which traverses the decision tree by falling under gravity. The pocket it falls into at the bottom determines which class it is assigned to. Each peg in the board represents a node in the decision tree that decides which path the data point should take down the board.\n",
    "\n",
    "![alt text](images/GaltonBoard.png)  \n",
    "_Figure 2.2 - Galton board_\n",
    "\n",
    "### 2.2 - Decision trees in the wild\n",
    "This definition allows for a wide range of use cases beyond the one detailed later. This concept is used frequently in game development to design the behaviour of non-player characters and enemies. This is clear in the board game Gloomhaven where players are given a decision tree to determine how the enemies move and attack the players. Each enemy has its own set of information (such as whether or not it is stunned, its position on the board, etc.) which can be considered as the features $(a_1,\\ a_2,\\ \\ldots ,\\ a_n)$ and the classification, $c$, is the action the enemy will take. The image below could be formalised to look more like figure 2.1 but I shall leave this as an exercise to the reader.\n",
    "\n",
    "![alt text](images/gloomhaven_behaviour_tree.png)  \n",
    "_Figure 2.3 - Gloomhaven behaviour tree_\n",
    "\n",
    "### 2.3 - Back to machine learning ###\n",
    "Now we return to the meat of this section: how decision trees are applied to machine learning. I will explore the \"learning\" as it applies here in the next section but first I will discuss some reasons why one might use decision trees over other models.\n",
    "\n",
    "Advantages:\n",
    "* Unlike other models (particularly neural networks), it is very easy to see how a decision tree comes to its conclusions.\n",
    "* As mentioned in §2.1, the data features can be either discrete or continuous.\n",
    "* Can be used in both classification and regression problems (though I'm only considering classification problems here).\n",
    "* Fast query time - on average this has complexity $O(\\log{n_{samples}})$[1].\n",
    "\n",
    "Disadvantages:\n",
    "* Use of a greedy approach (the best choice is chosen at each stage without considering the effect on future choices). This can lead to a non-optimal solution but will be faster than a more comprehensive search.\n",
    "* Small variations in the input data can lead to wildly different trees.\n",
    "* Slow construction time - on average this has complexity $O(n_{samples}n_{features}\\log(n_{samples}))$[1]. This issue can be mitigated since we only construct the tree once then query it from then on.\n",
    "* Cannot add data to the tree once constructed - unlike KNN, our model cannot be improved once constructed.\n",
    "* Prone to overfitting - (see Tobey's section for a more detailed description of the issue). Some remedies to this are discussed at the end of §2.4.\n",
    "\n",
    "\n",
    "### 2.4 - Algorithm for constructing a decision tree ###\n",
    "Now that we know what a decision tree looks like, can we construct one? Below I detail the algorithm for constructing a decision tree, italicised lines will be expanded upon later:\n",
    ">1. Receive a dataset. _Select stopping requirements_.\n",
    ">\n",
    ">2. Create a root node, assign the whole dataset to it and mark it as incomplete and active.\n",
    ">\n",
    ">3. _Choose a condition on which to split the data assigned to the active node_.\n",
    ">\n",
    ">4. Assign the chosen condition to the active node.\n",
    ">\n",
    ">5. Create two child nodes of the active node and mark each as incomplete and inactive.\n",
    ">\n",
    ">6. Assign the data that satisfies the condition to the first child and the remaining data to the second.\n",
    ">\n",
    ">7. Mark the active node as complete and inactive.\n",
    ">\n",
    ">8. If the stopping requirements have been met proceed to step 11, otherwise proceed to step 9.\n",
    ">\n",
    ">9. Select any incomplete node and set its status to active.\n",
    ">\n",
    ">10. Proceed to step 3.\n",
    ">\n",
    ">11. Select any incomplete node, mark it as complete and set its value to the mode class in its subset of the dataset. It is now a \n",
    ">leaf node.\n",
    ">\n",
    ">12. If all nodes are complete proceed to step 13, otherwise proceed to step 11.\n",
    ">\n",
    ">13. Terminate the algorithm.\n",
    "\n",
    "This algorithm can be optimised using recursion (which I will use in my code), though for clarity I use this slightly more inefficient version here.\n",
    "\n",
    "As promised, I will expand on step 1 now. Step 3 will be left for its own section.\n",
    "\n",
    "Step 1 is used as a method of preventing overfitting as well as enabling termination of the algorithm. There are two requirements that have to be met in order to terminate the algorithm:\n",
    "1. Maximum distance from the root - this prevents the algorithm continuing until all data has been classified perfectly.\n",
    "2. Minimum sample size - if the data subset assigned to a node has too few elements then we stop the algorithm to prevent overfitting.\n",
    "\n",
    "I will now explain step 3 of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3 - Choosing Optimal Splitting Conditions\n",
    "---\n",
    "\n",
    "### Information theory and entropy\n",
    "Given two choices of splitting conditions, how do we evaluate which is more effective at splitting the data. For this problem, \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "E(x) = \\sum_{c\\in C} -(p_c) log_2(p_c)\n",
    "\\end{align}\n",
    "Where $p_c$ is the probability of class $c$.\\\n",
    "$IG = Entropy(Parent) - \\sum w_iEntropy(Child)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4 - The Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30279</td>\n",
       "      <td>634.927</td>\n",
       "      <td>212.560556</td>\n",
       "      <td>181.510182</td>\n",
       "      <td>1.171067</td>\n",
       "      <td>0.520401</td>\n",
       "      <td>30600</td>\n",
       "      <td>196.347702</td>\n",
       "      <td>0.775688</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.943852</td>\n",
       "      <td>0.923726</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30477</td>\n",
       "      <td>670.033</td>\n",
       "      <td>211.050155</td>\n",
       "      <td>184.039050</td>\n",
       "      <td>1.146768</td>\n",
       "      <td>0.489478</td>\n",
       "      <td>30970</td>\n",
       "      <td>196.988633</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.984081</td>\n",
       "      <td>0.853080</td>\n",
       "      <td>0.933374</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.871186</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30519</td>\n",
       "      <td>629.727</td>\n",
       "      <td>212.996755</td>\n",
       "      <td>182.737204</td>\n",
       "      <td>1.165591</td>\n",
       "      <td>0.513760</td>\n",
       "      <td>30847</td>\n",
       "      <td>197.124320</td>\n",
       "      <td>0.770682</td>\n",
       "      <td>0.989367</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.925480</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.856514</td>\n",
       "      <td>0.998345</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30685</td>\n",
       "      <td>635.681</td>\n",
       "      <td>213.534145</td>\n",
       "      <td>183.157146</td>\n",
       "      <td>1.165852</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>31044</td>\n",
       "      <td>197.659696</td>\n",
       "      <td>0.771561</td>\n",
       "      <td>0.988436</td>\n",
       "      <td>0.954240</td>\n",
       "      <td>0.925658</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.856844</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30834</td>\n",
       "      <td>631.934</td>\n",
       "      <td>217.227813</td>\n",
       "      <td>180.897469</td>\n",
       "      <td>1.200834</td>\n",
       "      <td>0.553642</td>\n",
       "      <td>31120</td>\n",
       "      <td>198.139012</td>\n",
       "      <td>0.783683</td>\n",
       "      <td>0.990810</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>0.912125</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.831973</td>\n",
       "      <td>0.999061</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "5  30279    634.927       212.560556       181.510182      1.171067   \n",
       "6  30477    670.033       211.050155       184.039050      1.146768   \n",
       "7  30519    629.727       212.996755       182.737204      1.165591   \n",
       "8  30685    635.681       213.534145       183.157146      1.165852   \n",
       "9  30834    631.934       217.227813       180.897469      1.200834   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "5      0.520401       30600     196.347702  0.775688  0.989510   0.943852   \n",
       "6      0.489478       30970     196.988633  0.762402  0.984081   0.853080   \n",
       "7      0.513760       30847     197.124320  0.770682  0.989367   0.967109   \n",
       "8      0.514081       31044     197.659696  0.771561  0.988436   0.954240   \n",
       "9      0.553642       31120     198.139012  0.783683  0.990810   0.970278   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  \n",
       "5     0.923726      0.007020      0.003153      0.853270      0.999236  SEKER  \n",
       "6     0.933374      0.006925      0.003242      0.871186      0.999049  SEKER  \n",
       "7     0.925480      0.006979      0.003158      0.856514      0.998345  SEKER  \n",
       "8     0.925658      0.006959      0.003152      0.856844      0.998953  SEKER  \n",
       "9     0.912125      0.007045      0.003008      0.831973      0.999061  SEKER  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type']\n",
    "# data = pd.read_csv('iris.data', sep=',', header=None, names=col_names)\n",
    "col_names = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', 'Roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4', 'Class']\n",
    "data = pd.read_csv('Dry_Bean_Dataset.data', sep=',', header=None, names=col_names)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        \n",
    "        # For decision nodes\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "\n",
    "        # For leaf nodes\n",
    "        self.value = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        \n",
    "        # initialise the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def BuildTree(self, dataset, curr_depth=0): \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "\n",
    "            # find the best split\n",
    "            best_split = self.GetBestSplit(dataset, num_samples, num_features)\n",
    "\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "\n",
    "                # recur left\n",
    "                left_subtree = self.BuildTree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "\n",
    "                # recur right\n",
    "                right_subtree = self.BuildTree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.CalculateLeafValue(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def GetBestSplit(self, dataset, num_samples, num_features):\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.Split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.InformationGain(y, left_y, right_y)\n",
    "                    # update the best split if necessary\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def Split(self, dataset, feature_index, threshold):\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def InformationGain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.GiniIndex(parent) - (weight_l*self.GiniIndex(l_child) + weight_r*self.GiniIndex(r_child))\n",
    "        else:\n",
    "            gain = self.Entropy(parent) - (weight_l*self.Entropy(l_child) + weight_r*self.Entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def Entropy(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def GiniIndex(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def CalculateLeafValue(self, Y):\n",
    "\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def PrintTree(self, tree=None, indent=\" \"):\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.PrintTree(tree.left, indent + indent)\n",
    "            \n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.PrintTree(tree.right, indent + indent)\n",
    "    \n",
    "    def Fit(self, X, Y):\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.BuildTree(dataset)\n",
    "    \n",
    "    def Predict(self, X):\n",
    "        preditions = [self.MakePrediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def MakePrediction(self, x, tree):\n",
    "\n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.MakePrediction(x, tree.left)\n",
    "        else:\n",
    "            return self.MakePrediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=1000, test_size=10, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2 <= 337.53295866964567 ? 0.7862887945200716\n",
      " left:X_3 <= 181.27760357155148 ? 0.6030206435481191\n",
      "  left:X_2 <= 274.49470044845833 ? 0.2503655606801769\n",
      "    left:X_2 <= 256.0466415892121 ? 0.07010591896916663\n",
      "        left:X_12 <= 0.0073734377548812 ? 0.05971828222415425\n",
      "                left:DERMASON\n",
      "                right:DERMASON\n",
      "        right:X_8 <= 0.7161057692307692 ? 0.07979388599982407\n",
      "                left:DERMASON\n",
      "                right:DERMASON\n",
      "    right:X_4 <= 1.7643227793589036 ? 0.47392066312979675\n",
      "        left:X_15 <= 0.9937465421501556 ? 0.2602739159159402\n",
      "                left:SIRA\n",
      "                right:DERMASON\n",
      "        right:X_9 <= 0.988102632710593 ? 0.5435644431995964\n",
      "                left:HOROZ\n",
      "                right:SIRA\n",
      "  right:X_13 <= 0.0021023489782095 ? 0.7381502273188227\n",
      "    left:X_3 <= 210.26952899304652 ? 0.2182950950516962\n",
      "        left:X_0 <= 42147.0 ? 0.16854484493241406\n",
      "                left:SIRA\n",
      "                right:SIRA\n",
      "        right:X_10 <= 0.8517061443254937 ? 0.45914791702724467\n",
      "                left:BARBUNYA\n",
      "                right:CALI\n",
      "    right:X_0 <= 49888.0 ? 0.13923299905509884\n",
      "        left:X_11 <= 0.8756633095557972 ? 0.10805086098607913\n",
      "                left:SEKER\n",
      "                right:SEKER\n",
      "        right:BARBUNYA\n",
      " right:X_12 <= 0.0060545105414746 ? 0.8308027883271507\n",
      "  left:X_0 <= 105542.0 ? 0.6468921475171102\n",
      "    left:X_10 <= 0.8070797228431212 ? 0.37244566946976854\n",
      "        left:X_15 <= 0.9850040046676748 ? 0.1982349640977661\n",
      "                left:CALI\n",
      "                right:BARBUNYA\n",
      "        right:X_15 <= 0.9950713779306382 ? 0.33580968983620874\n",
      "                left:CALI\n",
      "                right:BARBUNYA\n",
      "    right:BOMBAY\n",
      "  right:X_13 <= 0.0012052830376182 ? 0.3181373885463612\n",
      "    left:X_9 <= 0.9772736178049448 ? 0.030143832266830983\n",
      "        left:X_1 <= 891.505 ? 0.4689955935892812\n",
      "                left:CALI\n",
      "                right:HOROZ\n",
      "        right:HOROZ\n",
      "    right:X_1 <= 896.53 ? 0.9544340029249649\n",
      "        left:SIRA\n",
      "        right:X_1 <= 921.455 ? 0.9182958340544896\n",
      "                left:CALI\n",
      "                right:BARBUNYA\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=4)\n",
    "classifier.Fit(X_train,Y_train)\n",
    "classifier.PrintTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = classifier.Predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5 - Potential Improvements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Add something about discrete data features and random forest, xgboost, etc.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glossary\n",
    "===\n",
    "\n",
    "Acyclic graph: A graph that contains no cycles.\n",
    "\n",
    "Arc: A line connecting two nodes on a graph.\n",
    "\n",
    "Binary tree: A tree where each node has at most two child nodes.\n",
    "\n",
    "Child node: A node with a parent.\n",
    "\n",
    "Connected nodes: Two nodes with an arc between them.\n",
    "\n",
    "Connected graph: A graph where there is a path between any pair of nodes.\n",
    "\n",
    "Cycle: A path that begins and ends at the same node.\n",
    "\n",
    "Cyclic graph: A graph that contains a cycle.\n",
    "\n",
    "Directed arc: An arc which can only be traversed in a single direction.\n",
    "\n",
    "Directed graph: A graph where at least one arc is directed.\n",
    "\n",
    "Graph: An ordered pair $(V, E)$ where $V$ is a set of vertices and $E$ a set of ordered pairs of vertices indicating connections between vertices.\n",
    "\n",
    "Leaf node: A node which has no child nodes.\n",
    "\n",
    "Node: A generic .\n",
    "\n",
    "Parent node: The first node reached on the path from a node to the root. Nodes can have at most one parent. The root node has no parents.\n",
    "\n",
    "Path: A sequence of nodes where all pairs of consecutive nodes are connected and no arc or node is repeated.\n",
    "\n",
    "Root node: A node that has been designated as the root.\n",
    "\n",
    "Tree: An acyclic, connected, undirected graph.\n",
    "\n",
    "Undirected arc: An arc which can be traversed in either direction.\n",
    "\n",
    "Undirected graph: A graph where all arcs are undirected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "---\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/tree.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
